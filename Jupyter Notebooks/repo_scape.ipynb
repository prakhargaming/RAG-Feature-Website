{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0538ba7c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb480d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pymongo\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "\n",
    "from typing import TypedDict\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "from pymongo.operations import SearchIndexModel\n",
    "from pymongo import collection\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb5d5e",
   "metadata": {},
   "source": [
    "## API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7e25bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_client = genai.Client(api_key=os.getenv(\"GEMINI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb050bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = os.getenv(\"MONGODB_URI\")\n",
    "mongo_client = pymongo.MongoClient(uri, server_api=pymongo.server_api.ServerApi(\n",
    "   version=\"1\", strict=False, deprecation_errors=True))\n",
    "\n",
    "Prakharbase = mongo_client[\"Prakharbase\"]\n",
    "vector_database = Prakharbase[\"vector_database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9694c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_USERNAME = \"prakhargaming\"\n",
    "GITHUB_TOKEN = os.getenv(\"REPO\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a349868",
   "metadata": {},
   "source": [
    "## Typed Dictionary for MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e06c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class repo(TypedDict):\n",
    "    name: str\n",
    "    url: str\n",
    "    languages: dict[str, int]\n",
    "    topics: list[str]\n",
    "    readme: str\n",
    "    embedding: list[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b14863",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1b2fe8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc(name=\"\", url=\"\", languages=\"\", tags=\"\", readme=\"\"):\n",
    "    return f\"\"\"\n",
    "# METADATA\n",
    "Repository name: {name}\n",
    "Repository URL: {url}\n",
    "Repository languages: {languages}\n",
    "Repository topics: {tags}\n",
    "\n",
    "# README:\n",
    "{readme}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf725a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tag(readme_text, languages) -> list[str]:\n",
    "    tags = []\n",
    "\n",
    "    # Keywords for different skill areas\n",
    "    computer_vision_keywords = [\"opencv\", \"cnn\", \"image\", \"vision\", \"detection\", \"segmentation\", \"recognition\"]\n",
    "    nlp_keywords = [\"bert\", \"transformer\", \"token\", \"nlp\", \"text classification\", \"language model\"]\n",
    "    web_dev_keywords = [\"react\", \"flask\", \"django\", \"express\", \"api\", \"frontend\", \"backend\", \"web app\"]\n",
    "    data_science_keywords = [\"pandas\", \"numpy\", \"dataframe\", \"analysis\", \"plot\", \"visualization\"]\n",
    "    ai_keywords = [\"deep learning\", \"machine learning\", \"reinforcement learning\", \"model\", \"training\"]\n",
    "\n",
    "    text = readme_text.lower()\n",
    "\n",
    "    # Helper function\n",
    "    def contains_any(keywords):\n",
    "        return any(keyword in text for keyword in keywords)\n",
    "\n",
    "    # Tagging based on content\n",
    "    if contains_any(computer_vision_keywords) or 'OpenCV' in languages:\n",
    "        tags.append(\"computer-vision\")\n",
    "    if contains_any(nlp_keywords):\n",
    "        tags.append(\"nlp\")\n",
    "    if contains_any(web_dev_keywords):\n",
    "        tags.append(\"web-development\")\n",
    "    if contains_any(data_science_keywords):\n",
    "        tags.append(\"data-science\")\n",
    "    if contains_any(ai_keywords):\n",
    "        tags.append(\"artificial-intelligence\")\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f367e14a",
   "metadata": {},
   "source": [
    "## Github Repository Scraper for RAG Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e1c166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_public_repo_information(username: str, generate_embeddings=False, generate_files=False) -> dict[str, repo]:\n",
    "    repo_url = f\"https://api.github.com/users/{username}/repos\"\n",
    "    request_repo = requests.get(repo_url, headers=headers)\n",
    "    if request_repo.status_code != 200:\n",
    "        print(f\"Request Failed (request_repo): {request_repo.status_code} \\n {repo_url}\")\n",
    "        return request_repo.status_code\n",
    "    data = request_repo.json()\n",
    "    repo_info = []\n",
    "    if generate_files:\n",
    "        directory = \"github_repos_info\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    for repos in data:\n",
    "        repo_name = repos[\"name\"]\n",
    "        repo_url = repos[\"url\"]\n",
    "        language_url = f\"https://api.github.com/repos/{username}/{repo_name}/languages\"\n",
    "        readme_url = f\"https://api.github.com/repos/{username}/{repo_name}/readme\"\n",
    "\n",
    "        request_languages = requests.get(language_url, headers=headers)\n",
    "        if request_languages.status_code == 200:     \n",
    "            repo_languages = request_languages.json()\n",
    "        else:\n",
    "            print(f\"Request Failed (request_languages): {request_languages.status_code} \\n {language_url}\")\n",
    "            repo_languages = {}\n",
    "\n",
    "        request_readme = requests.get(readme_url, headers=headers)\n",
    "        if request_readme.status_code == 200:\n",
    "            readme_content = request_readme.json()\n",
    "            repo_readme = base64.b64decode(readme_content[\"content\"]).decode('utf-8')\n",
    "        else:\n",
    "            print(f\"Request Failed (request_readme): {request_readme.status_code} \\n {readme_url}\")\n",
    "            repo_readme = \"\"\n",
    "        \n",
    "        repo_tags = auto_tag(repo_readme, repo_languages)\n",
    "        \n",
    "        if generate_embeddings:\n",
    "            to_embed = generate_desc(repo_name, repo_url, repo_languages, repo_tags, repo_readme)\n",
    "            result = google_client.models.embed_content(\n",
    "                model=\"text-embedding-004\",\n",
    "                contents=to_embed,\n",
    "                config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n",
    "            )\n",
    "            repo_embedding = result.embeddings[0].values\n",
    "        else:\n",
    "            repo_embedding = []\n",
    "\n",
    "        if generate_files:\n",
    "            file_path = f\"github_repos_info\\\\REPO_INFO_{repo_name}.txt\"\n",
    "            file_contents = generate_desc(repo_name, repo_url, repo_languages, repo_tags, repo_readme)\n",
    "            try:\n",
    "                with open(file_path, \"w\") as file:\n",
    "                    file.write(file_contents)\n",
    "                print(f\"File '{file_path}' created successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "        repo_info.append(\n",
    "            repo(\n",
    "                name=repo_name,\n",
    "                url=repo_url,\n",
    "                languages=repo_languages,\n",
    "                topics=auto_tag(repo_readme, repo_languages),\n",
    "                readme=repo_readme,\n",
    "                embedding=repo_embedding\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return repo_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dabce888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query: str) -> list[repo]:\n",
    "    documents = []\n",
    "    query_embedding_response = google_client.models.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        contents=query,\n",
    "        config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n",
    "    )\n",
    "    query_embedding = query_embedding_response.embeddings[0].values\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"exact\": True,\n",
    "                \"limit\": 5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                    \"_id\": 0,\n",
    "                    \"name\": 1,\n",
    "                    \"readme\": 1,\n",
    "                    \"topics\": 1,\n",
    "                    \"languages\": 1,\n",
    "                    \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    results = vector_database.aggregate(pipeline)\n",
    "    for r in results:\n",
    "        documents.append(generate_desc(name=r[\"name\"], languages=r[\"languages\"], tags=r[\"topics\"], readme=r[\"readme\"]))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c696a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\n# METADATA\\nRepository name: prakhargaming\\nRepository URL: \\nRepository languages: {}\\nRepository topics: ['computer-vision', 'web-development', 'data-science', 'artificial-intelligence']\\n\\n# README:\\n## Hi Everyone üôã\\u200d‚ôÇÔ∏è\\n\\nMy name is Prakhar Sinha. I am an aspiring software developer! I recently graduated from UC Davis and am looking to break into the software industry. A little bit about myself is that I love nature, traveling and going on hikes! However, when I'm inside, you can find me either reading a book, playing a video game or drawing. When it comes to CS, I have three key areas of focus.\\n\\n## ü§ñ **AI/Computer Vision** \\nI really like messing around with AI Models! I have a lot of experience here. My main area of interest in the field is **eXplainable AI (XAI)** and **Computer Vision**. Refer to these projects if you'd like to see what I've done in these areas!\\n- [FastSAM for Needle Biopsy:](https://github.com/prakhargaming/FastSAM-needle-biopsy) This is what I'm currently working on for UC Davis Health and Dr. Fereidouni's lab. I'm using the FastSAM model (originally developed by Meta Research) to develop an automated imaging processing pipeline for the [GigaFIBI microscope](https://opg.optica.org/abstract.cfm?uri=Microscopy-2024-MS1A.2).\\n- [XAI ResNet-50 Data Visualization Web Development Project:](https://github.com/prakhargaming/Data-Visualization-Web-Dev-Project) My task with this project was to visualize the second to last layer of the ResNet-50 machine-learning model to help us better understand why the model came to the designs it did.\\n\\n## üï∏ **Front-End Web Development** \\nI used to fear front-end development but now over half my commit history has to do with TypeScript üò®. I've grown to like it quite a bit! These days I'm usually working with NextJS, TypeScript and Tailwind.\\n- [Scripta Welcome Page:](https://github.com/prakhargaming/scripta-welcome-interface) This is what I'm currently working on as well! This is being developed for the company I intern at, VDart. I think it showcases my front-end skills the best. This web app prompts the user to record audio, works with AWS and DigitalOcean to send and receive data, uses Firebase for authentication and contains some of my most sophisticated front-end code to date.\\n- [Scripta Dashboard:](https://github.com/prakhargaming/scripta-dashboard) This repo follows the same story as the previous one. Cool things about this web app is that it uses TailwindUI for stylization (so it looks really nice), Firebase for authentication and dynamically renders tables with data from DigitalOcean.\\n  \\n## üß† **Brain-Computer Interfaces** \\nThis was my labor of love in college. I was the co-head of the Neurotech@Davis projects division for 2 years. I will always look fondly on my days developing brain computer interfaces in the future and I hope to one day go back to them in the industry.\\n- [Neuro-Prosthetic EEG Controlled Robotic Arm:](https://github.com/Neurotech-Davis/RoboticArm) I think this is the coolest thing I've ever made. This project used the mental imagery of a real person to move a prosthetic arm.\\n- [Chrome No Internet Game Using EEG](https://github.com/Neurotech-Davis/Neurofest-Project-2023) This was one of my first BCI projects. It used EEG signals to control the dinosaur in the famous chrome no-internet game.\\n\", '\\n# METADATA\\nRepository name: SimCLR_for_Fast_Learning\\nRepository URL: \\nRepository languages: {\\'HTML\\': 822219, \\'Jupyter Notebook\\': 563160, \\'Python\\': 17620}\\nRepository topics: [\\'computer-vision\\', \\'data-science\\', \\'artificial-intelligence\\']\\n\\n# README:\\n# SimCLR for Fast Learning\\nBy Prakhar Sinha\\n\\n## Introduction\\n*SimCLR for Fast Learning* is a fine-tuned version of the original [*PyTorch SimCLR: A Simple Framework for Contrastive Learning of Visual Representations*](https://github.com/sthalles/SimCLR) model trained on the firefighting device detection dataset on [Roboflow](firefighting-device-detection/). \\n\\n## Evaluation\\n<p align=\"middle\">\\n  <img src=\"README_images/train.png\" width=\"45%\" />\\n  <img src=\"README_images/test.png\" width=\"45%\" /> \\n</p>\\n\\nThe final accuracy of the model on the test dataset was $\\\\approx 0.8$. TSNE analysis was used to visualize the embedding space.\\n\\n## Hardware/Enviornment\\nThis model was trained on the ASUS Zephyrus G14 GA401IV notebook running Windows 11, equipped with a AMD Ryzen 9 4900HS processor and RTX 2060 Max-Q (6 GB vRAM) graphics card in 8 minutes. \\n\\n## Instructions\\n1. Open the terminal and run install all the neccessary libraries \\n   ```bash\\n   pip install -r requirements.txt\\n   ```\\n2. Download the [Roboflow dataset](https://universe.roboflow.com/ds/mPFMlmd3O4?key=7Z5xZILK4P). in YOLOv11 format. Put the dataset in the `./datasets` folder. These folders should now exist: `./datasets/test`, `./datasets/train`, and `./datasets/valid`\\n3. Run `python3 Symbol_Extraction.py`. This was extract all the symbols from the dataset and put them in a new folder called `./datasets_extracted_symbols`.\\n4. Run all the cells in `./SimCLR.ipynb`\\n\\n## Summary of Changes and Fine-Tuning\\n### `./Symbol_Extraction.py`\\nThis is a script that was created for this project. It reads the YOLOv11 annotations and extracts all the symbols from the dataset.\\n\\n### `./data_aug/contrastive_learning_dataset.py`\\nThe original classes were heavily modifed for the purposes for this project. They are now specialized to load in the symbols from `Symbol_Extraction.py`\\n\\n### Fine-Tuning and SimCLR Modifications\\n#### Hyperparameters\\nThis is the list of the hyperparameters that were used for this project\\n```py\\nbatch_size = 512  \\nepochs = 30      \\nlearning_rate = 0.001  \\ntemperature = 0.1  \\ndevice = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\nn_views = 2      \\nout_dim = 16     \\ndisable_cuda = False\\nfp16_precision = True  \\nlog_every_n_steps = 25 \\narch = \\'resnet18\\'\\nnum_workers = 4 \\nweight_decay = 2e-4\\n```\\n- `batch_size` Batch size was adjusted for 512 based on vRAM limitations.\\n- `epochs` On average, it took 7 minutes to train the model for 30 epochs, which I felt was reasonable.\\n- `learning_rate` I didn\\'t mess around with the learning rate too much. It is lower than usual to accomodate for time constraints.\\n- `n_views` The base model of SimCLR only supports 2 views to my knowledge.\\n- `out_dim` I kept this very small because the symbols themselves were, at most, 30x30 pixels.\\n- `arch` I chose `resnet18` because it was included with the base model. In the future, I might try a smaller version of resnet to see if that would effect anything.\\n- `weight_decay` This was set to $2e^{-4}$ because of the time constraints associated with this project.\\n\\n#### SimCLR modifications\\nI modified the SimCLR image transformation pipeline a signifcant amount because I wanted to specialize it to the task at hand. This is a list of transformations. This can be found in `./data_aug/contrastive_learning_dataset.py`:\\n```py\\ndef get_simclr_pipeline_transform(size, s=1):\\n    \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\\n    data_transforms = transforms.Compose([\\n        transforms.RandomResizedCrop(size=size),\\n        transforms.RandomHorizontalFlip(),\\n        transforms.RandomGrayscale(p=0.2),\\n        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\\n        transforms.ToTensor()\\n    ])\\n    return data_transforms\\n```\\n- `color_jitter` was removed entirely. Based on the nature of the symbols we were working with, I hypotheosized that adding color jitter to the transformations was unnessessary.\\n- `gaussian_blur` was also removed. Blurring such a small image didn\\'t seem to add anything and I thought it was confounding the model.\\n- `RandomAffine` was added. I though introducing pixel level shifts might improve the robustness of the model.\\n\\nThe rest of thr transforms are apart of the standard SimCLR pipeline.\\n\\n## Use of Generative AI\\nMethods for data visualization, towards the end of the Jupyter Notebook, were generated using Claude 3.5 Sonnet.\\n', \"\\n# METADATA\\nRepository name: GenAI_Catagorization_Engine\\nRepository URL: \\nRepository languages: {'Jupyter Notebook': 78858, 'Python': 46847, 'TypeScript': 10727, 'Shell': 837, 'JavaScript': 528, 'CSS': 345}\\nRepository topics: ['nlp', 'web-development', 'artificial-intelligence']\\n\\n# README:\\n# GenAI Powered Smart Categorization Engine\\nDeveloped by Prakhar Sinha\\n\\n## Introduction/Description\\nThis is a full-stack web application designed to ingest the following data as structured JSON data:\\n1. **Product Title**\\n2. **Product Description**\\n3. **Associated Search Results** \\n\\nThe engine will then output the search results that are actually relevant to the product title and description using GenAI and natural language processing (NLP).\\n\\n## Data Processing Pipeline\\nData is processed in a multistage pipeline:\\n1. JSON data is loaded into the system\\n2. Data is preprocessed according to standard NLP principles\\n3. Embeddings are generated\\n4. Heuristics are applied\\n5. Heuristics are cross-validated by an LLM\\n\\n### Data Preprocessing\\nThe data that is uploaded undergoes extensive preprocessing to make it ready for the embedding. Text is standarized by a combination of lowercasing, removing special characters, stopwords, and lemmatizing.\\n\\n### Embeddings\\nEmbeddings are generated from preprocessed data by one of two models:\\n1. **OpenAI text-embedding-3-small** This is the standard model and provides the best results\\n2. **Sentence Transformer all-MiniLM-L6-v2** This model is used if there are no OpenAI tokens available to use\\n\\n### Heuristics\\nThe main heuristic applied is **Cosine Similarity** where each search result is compared to its corresponding product title and description. This works very well most of the time and made me question the need to incorporate GenAI,\\n\\n### LLM/GenAI Cross-Validation\\nFinally, the search results are cross-validated by an LLM. I originally opted to go for GPT-4 but after testing this is felt overkill. I ended up using a lighter model, GPT-4o Mini, with mixed results. \\n\\n## Full-Stack Application\\nAlthough only the development of a GenAI pipeline was asked in the project description, I opted to go the extra mile and design a full stack application around the pipeline as well.\\n\\n### Tech Stack\\nThese are the technologies I used in the development of this project\\n\\n- **Frontend**\\n  - Next.js\\n  - TypeScript\\n  - Tailwind CSS\\n- **Backend**\\n  - Python\\n  - Flask\\n- **Communication**\\n  - Socket.io\\n  - JSON\\n\\n## Known Bugs\\n- The first and most noticeable bug is that you must refresh the frontend webpage whenever you want to upload a new JSON file. I'm working on fixing this.\\n\\n## Getting Started\\nThere is a helpful bash script to help you run this script locally. You will need to make an `.env` file with an OpenAI key in the `./backend/app/engine` directory to get started. After this is done, installation should be straight forward. Run this command in your terminal to begin.\\n\\n```sh\\n~ ./run.sh  \\n```\\n\", \"\\n# METADATA\\nRepository name: Neurotech-Davis2022\\nRepository URL: \\nRepository languages: {'Jupyter Notebook': 27360}\\nRepository topics: []\\n\\n# README:\\n# Neurotech-Davis2022\\n\\nhi lol\\n\", '\\n# METADATA\\nRepository name: transdeeplab-for-needle-biopsy\\nRepository URL: \\nRepository languages: {\\'Python\\': 203439, \\'Shell\\': 501}\\nRepository topics: [\\'computer-vision\\', \\'artificial-intelligence\\']\\n\\n# README:\\n# TransDeepLab For Needle Biopsy Image Segmentation\\n\\n(THIS IS A FORK OF THE ORIGINAL [TRANSDEEPLAB REPOSITORY](https://github.com/rezazad68/transdeeplab), ALL CREDITS GO TO THEM FOR THEIR ORIGINAL IMPLEMENTATION)\\n\\n## Few notes about usage:\\n\\n### `runImageThroughModel.py`\\n\\nThis python script produces an image segmentation mask with the trained ML model based on an image that you feed into it. This is how you use the script:\\n```bash\\npython3 script.py --image_path \"mask_generation/fun_images/217613113.png\"\\n```\\n\\n### `mask_maker_script.py`\\n\\nThis python script produces masks based on a given annotation file. This is how you use it:\\n```bash \\npython script_name.py --image_dir \"./mask_generation/fun_images\" --annotation_file \"./mask_generation/annotations1.csv\" --mask_dir ./\"mask_generation/masks\"\\n``` \\nThe only required argument is the annotation file. ']\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_context(\"is prakhar good at computer vision?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18786de7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7c5b6bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Failed (request_readme): 404 \n",
      " https://api.github.com/repos/prakhargaming/amazonInterview/readme\n",
      "File 'github_repos_info\\REPO_INFO_amazonInterview.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_Data-Visualization-Web-Dev-Project.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_FastSAM-needle-biopsy.txt' created successfully.\n",
      "An error occurred: 'charmap' codec can't encode character '\\u0259' in position 1143: character maps to <undefined>\n",
      "File 'github_repos_info\\REPO_INFO_flask-react-template.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_GenAI_Catagorization_Engine.txt' created successfully.\n",
      "Request Failed (request_readme): 404 \n",
      " https://api.github.com/repos/prakhargaming/Lab-thingy/readme\n",
      "File 'github_repos_info\\REPO_INFO_Lab-thingy.txt' created successfully.\n",
      "Request Failed (request_readme): 404 \n",
      " https://api.github.com/repos/prakhargaming/musicdiscordplaylistbot/readme\n",
      "File 'github_repos_info\\REPO_INFO_musicdiscordplaylistbot.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_Neurotech-Davis2022.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_Neurotech_at_Davis_Robotic_Arm.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_ntx22-ui.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_prakhar-website.txt' created successfully.\n",
      "An error occurred: 'charmap' codec can't encode characters in position 276-279: character maps to <undefined>\n",
      "File 'github_repos_info\\REPO_INFO_RAG-Feature-Website.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_scripta-dashboard.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_scripta-dashboard-2.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_scripta-welcome-interface.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_SimCLR_for_Fast_Learning.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_transdeeplab-for-needle-biopsy.txt' created successfully.\n",
      "File 'github_repos_info\\REPO_INFO_u_net-for-needle-biopsy.txt' created successfully.\n",
      "An error occurred: 'charmap' codec can't encode character '\\U0001f4da' in position 996: character maps to <undefined>\n",
      "File 'github_repos_info\\REPO_INFO_vidilab-website.txt' created successfully.\n",
      "Request Failed (request_readme): 404 \n",
      " https://api.github.com/repos/prakhargaming/websocker-react-app/readme\n",
      "File 'github_repos_info\\REPO_INFO_websocker-react-app.txt' created successfully.\n"
     ]
    }
   ],
   "source": [
    "repos = fetch_public_repo_information(GITHUB_USERNAME, generate_embeddings=True, generate_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "563ee221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertManyResult([ObjectId('681119164e98ea887be6b50d'), ObjectId('681119164e98ea887be6b50e'), ObjectId('681119164e98ea887be6b50f'), ObjectId('681119164e98ea887be6b510'), ObjectId('681119164e98ea887be6b511'), ObjectId('681119164e98ea887be6b512'), ObjectId('681119164e98ea887be6b513'), ObjectId('681119164e98ea887be6b514'), ObjectId('681119164e98ea887be6b515'), ObjectId('681119164e98ea887be6b516'), ObjectId('681119164e98ea887be6b517'), ObjectId('681119164e98ea887be6b518'), ObjectId('681119164e98ea887be6b519'), ObjectId('681119164e98ea887be6b51a'), ObjectId('681119164e98ea887be6b51b'), ObjectId('681119164e98ea887be6b51c'), ObjectId('681119164e98ea887be6b51d'), ObjectId('681119164e98ea887be6b51e'), ObjectId('681119164e98ea887be6b51f'), ObjectId('681119164e98ea887be6b520'), ObjectId('681119164e98ea887be6b521'), ObjectId('681119164e98ea887be6b522'), ObjectId('681119164e98ea887be6b523')], acknowledged=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_database.insert_many(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea2e48d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vector_index'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_index_model = SearchIndexModel(\n",
    "    definition={\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"similarity\": \"cosine\",  \n",
    "                \"numDimensions\": 768\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    name=\"vector_index\",\n",
    "    type=\"vectorSearch\"\n",
    ")\n",
    "\n",
    "vector_database.create_search_index(model=search_index_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3b363475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# METADATA\n",
      "Repository name: vidilab-website\n",
      "Repository URL: \n",
      "Repository languages: {'Python': 6283, 'JavaScript': 3751, 'HTML': 1721, 'Dockerfile': 1442, 'CSS': 1260, 'Shell': 361, 'Makefile': 143}\n",
      "Repository topics: []\n",
      "\n",
      "# README:\n",
      "### VIDI-LAB Website\n",
      "\n",
      "WIP\n",
      "\n",
      "# METADATA\n",
      "Repository name: Data-Visualization-Web-Dev-Project\n",
      "Repository URL: \n",
      "Repository languages: {'JavaScript': 38274, 'Jupyter Notebook': 34710, 'Python': 12521, 'HTML': 1939, 'CSS': 1191}\n",
      "Repository topics: ['computer-vision', 'web-development', 'data-science', 'artificial-intelligence']\n",
      "\n",
      "# README:\n",
      "# Data Visualization Web Development Project for VIDI Reseach Lab at UC Davis\n",
      "\n",
      "This was a data visualization web development project that used a React, SocketIO, Flask/Python tech stack. This was a machine learning adjacent project as well. It was very informative in helping develop web development skills on a consistent deadline. Through development of this web app, I learned a lot of new skills:\n",
      "-   Front end was developed using React, D3 for data visualization, and MUI\n",
      "-\tCommunication is handled by Socket.io and JSON\n",
      "-\tBack end was developed using Flask, Python, PyTorch, NumPy, and OpenCV\n",
      "-\tMachine learning/Data Visualization aspect was visualizing the second to last layer of the ResNet-50 network with the use of forward hooks and TSNE analysis\n",
      "\n",
      "To run this application, please follow these steps (please note, the data vizualization aspect will not work since the ML waterbirds dataset is not included in this repo):\n",
      "1)  Clone the repo \n",
      "2)  `cd` into the repo and input these commands: \n",
      "    ```bash\n",
      "        python3 -m venv env  \n",
      "        source env/bin/activate  \n",
      "        pip install -r requirements.txt\n",
      "    ```\n",
      "3) `cd` into the front end folder and run these commands: \n",
      "    ```bash\n",
      "        npm i react-scripts\n",
      "    ```\n",
      "4) To run locally, split your terminal. Input these commands at the root of the application in the first terminal window: \n",
      "    ```bash  \n",
      "        cd webSocket-App  \n",
      "        source env/bin/activate  \n",
      "        python3 server.py\n",
      "    ```\n",
      "5) In the second terminal window: \n",
      "    ```bash  \n",
      "        cd webSocket-App/front-end  \n",
      "        npm start\n",
      "    ```\n",
      "\n",
      "Here is a GIF of the web app working in action. \\\n",
      "![](/applicationDemo.gif)\n",
      "\n",
      "# METADATA\n",
      "Repository name: websocker-react-app\n",
      "Repository URL: \n",
      "Repository languages: {'JavaScript': 13656, 'Python': 1188, 'Dockerfile': 854, 'HTML': 656, 'Shell': 145, 'Makefile': 143}\n",
      "Repository topics: []\n",
      "\n",
      "# README:\n",
      "\n",
      "\n",
      "# METADATA\n",
      "Repository name: scripta-welcome-interface\n",
      "Repository URL: \n",
      "Repository languages: {'TypeScript': 45328, 'CSS': 6062, 'JavaScript': 1431}\n",
      "Repository topics: []\n",
      "\n",
      "# README:\n",
      "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "First, run the development server:\n",
      "\n",
      "```bash\n",
      "npm run dev\n",
      "```\n",
      "\n",
      "Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n",
      "\n",
      "## Different Pages and Figma\n",
      "This is the [link](https://www.figma.com/design/4TTKiZBWNj3BVHnpH7McFs/VDart-Figma?node-id=1439-126&t=E4HT0VewWMuu1kLp-0) to the Figma design. All .tsx pages can be found in the `./src/pages` directory. Every `[name].tsx` page in the `./src/pages` directory corresponds to it's respective page in the Figma design. I am still working on stylization fixes and as well as generally optimizing the code.\n",
      "\n",
      "# METADATA\n",
      "Repository name: scripta-dashboard-2\n",
      "Repository URL: \n",
      "Repository languages: {'TypeScript': 32518, 'CSS': 1361, 'JavaScript': 925}\n",
      "Repository topics: []\n",
      "\n",
      "# README:\n",
      "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "First, run the development server:\n",
      "\n",
      "```bash\n",
      "npm run dev\n",
      "```\n",
      "\n",
      "Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n",
      "\n",
      "## Different Pages and Figma\n",
      "This is the [link](https://www.figma.com/design/4TTKiZBWNj3BVHnpH7McFs/VDart-Figma?node-id=1439-126&t=E4HT0VewWMuu1kLp-0) to the Figma design. The design, at the moment, is all contained within the `./src/app/page.tsx` file. \n"
     ]
    }
   ],
   "source": [
    "# Create query embedding\n",
    "to_embed = \"web dev\"\n",
    "result = google_client.models.embed_content(\n",
    "    model=\"text-embedding-004\",\n",
    "    contents=to_embed,\n",
    "    config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n",
    ")\n",
    "query_embedding = result.embeddings[0].values\n",
    "\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"path\": \"embedding\",\n",
    "            \"exact\": True,\n",
    "            \"limit\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"name\": 1,\n",
    "                \"readme\": 1,\n",
    "                \"topics\": 1,\n",
    "                \"languages\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the query\n",
    "results = vector_database.aggregate(pipeline)\n",
    "\n",
    "# Display results\n",
    "for r in results:\n",
    "    print(generate_desc(name=r[\"name\"], languages=r[\"languages\"], tags=r[\"topics\"], readme=r[\"readme\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc7180",
   "metadata": {},
   "source": [
    "## hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "44f209e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an AI chatbot that helps users learn about\n",
    "                   Prakhar Sinha, a software engineer specializing in\n",
    "                   AI/ML, Front-End, and BCI projects. You will \n",
    "                   primarily be responding towards recruiters and his\n",
    "                   peers, so make sure you make him look good. \n",
    "                    \n",
    "                   Answer the provided query based on the provided context and\n",
    "                   provide evidence to support your claim.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7766b92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Prakhar is skilled in computer vision. His GitHub repositories, like \"FastSAM for Needle Biopsy\" and \"TransDeepLab For Needle Biopsy Image Segmentation\", demonstrate his experience with image segmentation and model implementation. Additionally, the \"XAI ResNet-50 Data Visualization Web Development Project\" shows his ability to visualize and understand complex AI models.\n",
      "\n",
      "Goodbye.\n",
      "role - user: Context: [\"\\n# METADATA\\nRepository name: prakhargaming\\nRepository URL: \\nRepository languages: {}\\nRepository topics: ['computer-vision', 'web-development', 'data-science', 'artificial-intelligence']\\n\\n# README:\\n## Hi Everyone üôã\\u200d‚ôÇÔ∏è\\n\\nMy name is Prakhar Sinha. I am an aspiring software developer! I recently graduated from UC Davis and am looking to break into the software industry. A little bit about myself is that I love nature, traveling and going on hikes! However, when I'm inside, you can find me either reading a book, playing a video game or drawing. When it comes to CS, I have three key areas of focus.\\n\\n## ü§ñ **AI/Computer Vision** \\nI really like messing around with AI Models! I have a lot of experience here. My main area of interest in the field is **eXplainable AI (XAI)** and **Computer Vision**. Refer to these projects if you'd like to see what I've done in these areas!\\n- [FastSAM for Needle Biopsy:](https://github.com/prakhargaming/FastSAM-needle-biopsy) This is what I'm currently working on for UC Davis Health and Dr. Fereidouni's lab. I'm using the FastSAM model (originally developed by Meta Research) to develop an automated imaging processing pipeline for the [GigaFIBI microscope](https://opg.optica.org/abstract.cfm?uri=Microscopy-2024-MS1A.2).\\n- [XAI ResNet-50 Data Visualization Web Development Project:](https://github.com/prakhargaming/Data-Visualization-Web-Dev-Project) My task with this project was to visualize the second to last layer of the ResNet-50 machine-learning model to help us better understand why the model came to the designs it did.\\n\\n## üï∏ **Front-End Web Development** \\nI used to fear front-end development but now over half my commit history has to do with TypeScript üò®. I've grown to like it quite a bit! These days I'm usually working with NextJS, TypeScript and Tailwind.\\n- [Scripta Welcome Page:](https://github.com/prakhargaming/scripta-welcome-interface) This is what I'm currently working on as well! This is being developed for the company I intern at, VDart. I think it showcases my front-end skills the best. This web app prompts the user to record audio, works with AWS and DigitalOcean to send and receive data, uses Firebase for authentication and contains some of my most sophisticated front-end code to date.\\n- [Scripta Dashboard:](https://github.com/prakhargaming/scripta-dashboard) This repo follows the same story as the previous one. Cool things about this web app is that it uses TailwindUI for stylization (so it looks really nice), Firebase for authentication and dynamically renders tables with data from DigitalOcean.\\n  \\n## üß† **Brain-Computer Interfaces** \\nThis was my labor of love in college. I was the co-head of the Neurotech@Davis projects division for 2 years. I will always look fondly on my days developing brain computer interfaces in the future and I hope to one day go back to them in the industry.\\n- [Neuro-Prosthetic EEG Controlled Robotic Arm:](https://github.com/Neurotech-Davis/RoboticArm) I think this is the coolest thing I've ever made. This project used the mental imagery of a real person to move a prosthetic arm.\\n- [Chrome No Internet Game Using EEG](https://github.com/Neurotech-Davis/Neurofest-Project-2023) This was one of my first BCI projects. It used EEG signals to control the dinosaur in the famous chrome no-internet game.\\n\", '\\n# METADATA\\nRepository name: SimCLR_for_Fast_Learning\\nRepository URL: \\nRepository languages: {\\'HTML\\': 822219, \\'Jupyter Notebook\\': 563160, \\'Python\\': 17620}\\nRepository topics: [\\'computer-vision\\', \\'data-science\\', \\'artificial-intelligence\\']\\n\\n# README:\\n# SimCLR for Fast Learning\\nBy Prakhar Sinha\\n\\n## Introduction\\n*SimCLR for Fast Learning* is a fine-tuned version of the original [*PyTorch SimCLR: A Simple Framework for Contrastive Learning of Visual Representations*](https://github.com/sthalles/SimCLR) model trained on the firefighting device detection dataset on [Roboflow](firefighting-device-detection/). \\n\\n## Evaluation\\n<p align=\"middle\">\\n  <img src=\"README_images/train.png\" width=\"45%\" />\\n  <img src=\"README_images/test.png\" width=\"45%\" /> \\n</p>\\n\\nThe final accuracy of the model on the test dataset was $\\\\approx 0.8$. TSNE analysis was used to visualize the embedding space.\\n\\n## Hardware/Enviornment\\nThis model was trained on the ASUS Zephyrus G14 GA401IV notebook running Windows 11, equipped with a AMD Ryzen 9 4900HS processor and RTX 2060 Max-Q (6 GB vRAM) graphics card in 8 minutes. \\n\\n## Instructions\\n1. Open the terminal and run install all the neccessary libraries \\n   ```bash\\n   pip install -r requirements.txt\\n   ```\\n2. Download the [Roboflow dataset](https://universe.roboflow.com/ds/mPFMlmd3O4?key=7Z5xZILK4P). in YOLOv11 format. Put the dataset in the `./datasets` folder. These folders should now exist: `./datasets/test`, `./datasets/train`, and `./datasets/valid`\\n3. Run `python3 Symbol_Extraction.py`. This was extract all the symbols from the dataset and put them in a new folder called `./datasets_extracted_symbols`.\\n4. Run all the cells in `./SimCLR.ipynb`\\n\\n## Summary of Changes and Fine-Tuning\\n### `./Symbol_Extraction.py`\\nThis is a script that was created for this project. It reads the YOLOv11 annotations and extracts all the symbols from the dataset.\\n\\n### `./data_aug/contrastive_learning_dataset.py`\\nThe original classes were heavily modifed for the purposes for this project. They are now specialized to load in the symbols from `Symbol_Extraction.py`\\n\\n### Fine-Tuning and SimCLR Modifications\\n#### Hyperparameters\\nThis is the list of the hyperparameters that were used for this project\\n```py\\nbatch_size = 512  \\nepochs = 30      \\nlearning_rate = 0.001  \\ntemperature = 0.1  \\ndevice = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\nn_views = 2      \\nout_dim = 16     \\ndisable_cuda = False\\nfp16_precision = True  \\nlog_every_n_steps = 25 \\narch = \\'resnet18\\'\\nnum_workers = 4 \\nweight_decay = 2e-4\\n```\\n- `batch_size` Batch size was adjusted for 512 based on vRAM limitations.\\n- `epochs` On average, it took 7 minutes to train the model for 30 epochs, which I felt was reasonable.\\n- `learning_rate` I didn\\'t mess around with the learning rate too much. It is lower than usual to accomodate for time constraints.\\n- `n_views` The base model of SimCLR only supports 2 views to my knowledge.\\n- `out_dim` I kept this very small because the symbols themselves were, at most, 30x30 pixels.\\n- `arch` I chose `resnet18` because it was included with the base model. In the future, I might try a smaller version of resnet to see if that would effect anything.\\n- `weight_decay` This was set to $2e^{-4}$ because of the time constraints associated with this project.\\n\\n#### SimCLR modifications\\nI modified the SimCLR image transformation pipeline a signifcant amount because I wanted to specialize it to the task at hand. This is a list of transformations. This can be found in `./data_aug/contrastive_learning_dataset.py`:\\n```py\\ndef get_simclr_pipeline_transform(size, s=1):\\n    \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\\n    data_transforms = transforms.Compose([\\n        transforms.RandomResizedCrop(size=size),\\n        transforms.RandomHorizontalFlip(),\\n        transforms.RandomGrayscale(p=0.2),\\n        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\\n        transforms.ToTensor()\\n    ])\\n    return data_transforms\\n```\\n- `color_jitter` was removed entirely. Based on the nature of the symbols we were working with, I hypotheosized that adding color jitter to the transformations was unnessessary.\\n- `gaussian_blur` was also removed. Blurring such a small image didn\\'t seem to add anything and I thought it was confounding the model.\\n- `RandomAffine` was added. I though introducing pixel level shifts might improve the robustness of the model.\\n\\nThe rest of thr transforms are apart of the standard SimCLR pipeline.\\n\\n## Use of Generative AI\\nMethods for data visualization, towards the end of the Jupyter Notebook, were generated using Claude 3.5 Sonnet.\\n', \"\\n# METADATA\\nRepository name: GenAI_Catagorization_Engine\\nRepository URL: \\nRepository languages: {'Jupyter Notebook': 78858, 'Python': 46847, 'TypeScript': 10727, 'Shell': 837, 'JavaScript': 528, 'CSS': 345}\\nRepository topics: ['nlp', 'web-development', 'artificial-intelligence']\\n\\n# README:\\n# GenAI Powered Smart Categorization Engine\\nDeveloped by Prakhar Sinha\\n\\n## Introduction/Description\\nThis is a full-stack web application designed to ingest the following data as structured JSON data:\\n1. **Product Title**\\n2. **Product Description**\\n3. **Associated Search Results** \\n\\nThe engine will then output the search results that are actually relevant to the product title and description using GenAI and natural language processing (NLP).\\n\\n## Data Processing Pipeline\\nData is processed in a multistage pipeline:\\n1. JSON data is loaded into the system\\n2. Data is preprocessed according to standard NLP principles\\n3. Embeddings are generated\\n4. Heuristics are applied\\n5. Heuristics are cross-validated by an LLM\\n\\n### Data Preprocessing\\nThe data that is uploaded undergoes extensive preprocessing to make it ready for the embedding. Text is standarized by a combination of lowercasing, removing special characters, stopwords, and lemmatizing.\\n\\n### Embeddings\\nEmbeddings are generated from preprocessed data by one of two models:\\n1. **OpenAI text-embedding-3-small** This is the standard model and provides the best results\\n2. **Sentence Transformer all-MiniLM-L6-v2** This model is used if there are no OpenAI tokens available to use\\n\\n### Heuristics\\nThe main heuristic applied is **Cosine Similarity** where each search result is compared to its corresponding product title and description. This works very well most of the time and made me question the need to incorporate GenAI,\\n\\n### LLM/GenAI Cross-Validation\\nFinally, the search results are cross-validated by an LLM. I originally opted to go for GPT-4 but after testing this is felt overkill. I ended up using a lighter model, GPT-4o Mini, with mixed results. \\n\\n## Full-Stack Application\\nAlthough only the development of a GenAI pipeline was asked in the project description, I opted to go the extra mile and design a full stack application around the pipeline as well.\\n\\n### Tech Stack\\nThese are the technologies I used in the development of this project\\n\\n- **Frontend**\\n  - Next.js\\n  - TypeScript\\n  - Tailwind CSS\\n- **Backend**\\n  - Python\\n  - Flask\\n- **Communication**\\n  - Socket.io\\n  - JSON\\n\\n## Known Bugs\\n- The first and most noticeable bug is that you must refresh the frontend webpage whenever you want to upload a new JSON file. I'm working on fixing this.\\n\\n## Getting Started\\nThere is a helpful bash script to help you run this script locally. You will need to make an `.env` file with an OpenAI key in the `./backend/app/engine` directory to get started. After this is done, installation should be straight forward. Run this command in your terminal to begin.\\n\\n```sh\\n~ ./run.sh  \\n```\\n\", \"\\n# METADATA\\nRepository name: Neurotech-Davis2022\\nRepository URL: \\nRepository languages: {'Jupyter Notebook': 27360}\\nRepository topics: []\\n\\n# README:\\n# Neurotech-Davis2022\\n\\nhi lol\\n\", '\\n# METADATA\\nRepository name: transdeeplab-for-needle-biopsy\\nRepository URL: \\nRepository languages: {\\'Python\\': 203439, \\'Shell\\': 501}\\nRepository topics: [\\'computer-vision\\', \\'artificial-intelligence\\']\\n\\n# README:\\n# TransDeepLab For Needle Biopsy Image Segmentation\\n\\n(THIS IS A FORK OF THE ORIGINAL [TRANSDEEPLAB REPOSITORY](https://github.com/rezazad68/transdeeplab), ALL CREDITS GO TO THEM FOR THEIR ORIGINAL IMPLEMENTATION)\\n\\n## Few notes about usage:\\n\\n### `runImageThroughModel.py`\\n\\nThis python script produces an image segmentation mask with the trained ML model based on an image that you feed into it. This is how you use the script:\\n```bash\\npython3 script.py --image_path \"mask_generation/fun_images/217613113.png\"\\n```\\n\\n### `mask_maker_script.py`\\n\\nThis python script produces masks based on a given annotation file. This is how you use it:\\n```bash \\npython script_name.py --image_dir \"./mask_generation/fun_images\" --annotation_file \"./mask_generation/annotations1.csv\" --mask_dir ./\"mask_generation/masks\"\\n``` \\nThe only required argument is the annotation file. '] \n",
      " Query: is prakhar good at computer vision?\n",
      "role - model: Yes, Prakhar is skilled in computer vision. His GitHub repositories, like \"FastSAM for Needle Biopsy\" and \"TransDeepLab For Needle Biopsy Image Segmentation\", demonstrate his experience with image segmentation and model implementation. Additionally, the \"XAI ResNet-50 Data Visualization Web Development Project\" shows his ability to visualize and understand complex AI models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = google_client.chats.create(model=\"gemini-2.0-flash\",\n",
    "                                  config=types.GenerateContentConfig(\n",
    "                                      system_instruction=system_prompt))\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input == \"EXIT\":\n",
    "        print(\"Goodbye.\")\n",
    "        break\n",
    "    \n",
    "    context = retrieve_context(user_input)\n",
    "    response = chat.send_message(f\"Context: {context} \\n Query: {user_input}\")\n",
    "    print(response.text)\n",
    "\n",
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}', end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
